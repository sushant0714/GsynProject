{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56a2f5ac-a55b-4dca-9f35-ea0aae459047",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "def check_null_values(df, df_name):\n",
    "    \"\"\"Checks for null values in each column.\"\"\"\n",
    "    print(f\"Checking null values for DataFrame: {df_name}\")\n",
    "    for col_name in df.columns:\n",
    "        null_count = df.filter(col(col_name).isNull()).count()\n",
    "        if null_count > 0:\n",
    "            print(f\" DataFrame '{df_name}': Column '{col_name}' has {null_count} null values\")\n",
    "    print(f\"Null Value check for DataFrame '{df_name}' is completed!\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c5d2f0a-825e-4a64-9ed2-0f6a05cc9b8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "def check_primary_key_uniqueness(df, df_name, primary_key):\n",
    "    \"\"\"Checks uniqueness of primary key.\"\"\"\n",
    "    print(f\"Checking uniqueness for primary key '{primary_key}' in DataFrame: {df_name}\")\n",
    "    total_count = df.count()\n",
    "    unique_count = df.select(countDistinct(col(primary_key))).collect()[0][0]\n",
    "    \n",
    "    if total_count == unique_count:\n",
    "        print(f\"Primary key '{primary_key}' is unique in '{df_name}'.\")\n",
    "    else:\n",
    "        print(f\"Primary key '{primary_key}' has duplicate values in '{df_name}'.\")\n",
    "    \n",
    "    print(f\"Primary Key Uniqueness check for '{df_name}' is completed!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d1a6e99-050c-4b45-acb8-4230b95456e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_data_types(df, df_name, expected_schema):\n",
    "    \"\"\"Validates the data types of columns.\"\"\"\n",
    "    print(f\"Checking data types for DataFrame: {df_name}\")\n",
    "    actual_schema = {col_name: dtype for col_name, dtype in df.dtypes}\n",
    "    \n",
    "    for col_name, expected_dtype in expected_schema.items():\n",
    "        actual_dtype = actual_schema.get(col_name)\n",
    "        if actual_dtype != expected_dtype:\n",
    "            print(f\" Column '{col_name}' in '{df_name}' has incorrect data type. Expected: {expected_dtype}, Found: {actual_dtype}\")\n",
    "        else:\n",
    "            print(f\"Column '{col_name}' in '{df_name}' has the correct data type: {expected_dtype}.\")\n",
    "    \n",
    "    print(f\"Data Type check for '{df_name}' is completed!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c8bd44b-4a01-40c5-a49f-1ecccb648af0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "def check_foreign_key_constraint(fact_df, fact_name, dim_df, dim_name, fact_fk, dim_pk):\n",
    "    \"\"\"Checks if foreign key values in fact table exist in dimension table.\"\"\"\n",
    "    print(f\"🔍 Checking Foreign Key Constraint between '{fact_name}' (fact) and '{dim_name}' (dimension) on '{fact_fk}' → '{dim_pk}'\")\n",
    "\n",
    "    # Find foreign key values in fact that are missing in dimension\n",
    "    missing_fk_count = fact_df.join(dim_df, fact_df[fact_fk] == dim_df[dim_pk], \"left_anti\").count()\n",
    "\n",
    "    if missing_fk_count > 0:\n",
    "        print(f\"{missing_fk_count} records in '{fact_name}' have missing foreign key values that do not exist in '{dim_name}'.\")\n",
    "    else:\n",
    "        print(f\"Foreign Key Constraint '{fact_fk} → {dim_pk}' is satisfied.\")\n",
    "\n",
    "    print(f\" Foreign Key Constraint check for '{fact_name}' → '{dim_name}' is completed!\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99e6a530-da44-4458-a09f-64bc7bab2d45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Basic Checks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}